#!/bin/sh
#SBATCH --partition=general-compute
#SBATCH --time=6:00:00
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=8
##SBATCH --mem=24000
# Memory per node specification is in MB. It is optional. 
# The default limit is 3000MB per core.
#SBATCH --job-name="adjoint"
#SBATCH --output=test-srun.out
#SBATCH --qos=supporters
#SBATCH --mail-user=haghakha@buffalo.edu
#SBATCH --mail-type=ALL
#SBATCH --exclusive
##SBATCH --requeue
#Specifies that the job will be requeued after a node failure.
#The default is that the job will not be requeued.
echo "SLURM_JOBID="$SLURM_JOBID
echo "SLURM_JOB_NODELIST"=$SLURM_JOB_NODELIST
echo "SLURM_NNODES"=$SLURM_NNODES
echo "SLURMTMPDIR="$SLURMTMPDIR

echo "working directory = "$SLURM_SUBMIT_DIR
module load intel-mpi  mkl
#ulimit -s unlimited
# The initial srun will trigger the SLURM prologue on the compute nodes.
NPROCS=`srun --nodes=${SLURM_NNODES} bash -c 'hostname' |wc -l`
echo NPROCS=$NPROCS
#The PMI library is necessary for srun
export I_MPI_PMI_LIBRARY=/usr/lib64/libpmi.so
tic=`date +%s`
srun ./titan
toc=`date +%s`

elapsedTime=`expr $toc - $tic`

echo "Elapsed Time = $elapsedTime seconds"

echo "All Done!"
